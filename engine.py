# File: engine.py
# Core TTS model loading and speech generation logic.
import os
os.environ["TRANSFORMERS_ATTN_IMPLEMENTATION"] = "eager"
import logging
import random
import numpy as np
import torch
import gc
from chatterbox.mtl_tts import ChatterboxMultilingualTTS
from typing import Optional, Tuple
from pathlib import Path
from chatterbox.tts import ChatterboxTTS  # Main TTS engine class
from chatterbox.models.s3gen.const import (
    S3GEN_SR,
)  # Default sample rate from the engine

# Import the singleton config_manager
from config import config_manager

logger = logging.getLogger(__name__)



from pathlib import Path
import torch
from safetensors.torch import load_file as load_safetensors

from chatterbox.models.t3 import T3
from chatterbox.models.t3.modules.t3_config import T3Config
from chatterbox.models.s3gen import S3Gen
from chatterbox.models.voice_encoder import VoiceEncoder
from chatterbox.models.tokenizers import MTLTokenizer
from chatterbox.mtl_tts import Conditionals, SUPPORTED_LANGUAGES # Need to import these too

from chatterbox.vc import ChatterboxVC

class PatchedChatterboxTTS(ChatterboxMultilingualTTS):
    """
    An inherited class that fixes the attention implementation issue by overriding
    the `from_local` class method.
    """
    @classmethod
    def from_local(cls, ckpt_dir, device) -> 'PatchedChatterboxTTS':
        print("ðŸš€ Using PatchedChatterboxTTS.from_local to load the model.")
        ckpt_dir = Path(ckpt_dir)

        # --- This is the original code from the library ---
        ve = VoiceEncoder()
        ve.load_state_dict(
            ###########torch.load(ckpt_dir / "ve.pt", weights_only=True)
            torch.load(ckpt_dir / "ve.pt", map_location=device, weights_only=True)
        )
        ve.to(device).eval()
        
        # --- OUR FIX IS APPLIED HERE ---
        # 1. Create the T3Config
        t3_config = T3Config.multilingual()
        
        # 2. Patch the config object directly
        # This part is slightly different because T3 doesn't take the config directly for attn
        # We'll go back to patching the LlamaConfig inside T3's init, but called from our override
        
        # Let's use the better approach of patching the T3's config logic
        # For simplicity, we directly recreate the T3 object with the fix logic
        from chatterbox.models.t3.llama_configs import LLAMA_CONFIGS
        from transformers import LlamaConfig, LlamaModel
        
        hp = T3Config.multilingual()
        cfg = LlamaConfig(**LLAMA_CONFIGS[hp.llama_config_name])
        cfg._attn_implementation = "eager" # Our patch
        
        # We need to manually recreate T3 since we can't inject the patched cfg easily
        # A simpler way is to just call the original method and then fix the model...
        # Let's try a cleaner override. We will replicate the method entirely.

        # The T3 class init needs to be fixed. So we create our own T3.
        class PatchedT3(T3):
            def __init__(self, hp=None):
                super().__init__(hp)
                # Override the transformer model with a patched config
                cfg = self.cfg
                cfg._attn_implementation = "eager"
                self.tfmr = LlamaModel(cfg)

        t3 = PatchedT3(T3Config.multilingual()) # Use our patched T3
        
        t3_state = load_safetensors(ckpt_dir / "t3_mtl23ls_v2.safetensors")
        if "model" in t3_state.keys():
            t3_state = t3_state["model"][0]
        t3.load_state_dict(t3_state)
        t3.to(device).eval()

        s3gen = S3Gen()
        s3gen.load_state_dict(
            ######torch.load(ckpt_dir / "s3gen.pt", weights_only=True)
            torch.load(ckpt_dir / "s3gen.pt", map_location=device, weights_only=True)
        )
        s3gen.to(device).eval()

        tokenizer = MTLTokenizer(
            str(ckpt_dir / "grapheme_mtl_merged_expanded_v1.json")
        )

        conds = None
        if (builtin_voice := ckpt_dir / "conds.pt").exists():
            conds = Conditionals.load(builtin_voice).to(device)

        return cls(t3, s3gen, ve, tokenizer, device, conds=conds)


# --- Global Module Variables ---
multilingual_model: Optional[PatchedChatterboxTTS] = None
MULTILINGUAL_MODEL_LOADED: bool = False
chatterbox_model: Optional[ChatterboxTTS] = None
MODEL_LOADED: bool = False
model_device: Optional[str] = (
    None  # Stores the resolved device string ('cuda' or 'cpu')
)

vc_model: Optional[ChatterboxVC] = None
VC_MODEL_LOADED: bool = False


def set_seed(seed_value: int):
    """
    Sets the seed for torch, random, and numpy for reproducibility.
    This is called if a non-zero seed is provided for generation.
    """
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)  # if using multi-GPU
    if torch.backends.mps.is_available():
        torch.mps.manual_seed(seed_value)
    random.seed(seed_value)
    np.random.seed(seed_value)
    logger.info(f"Global seed set to: {seed_value}")


def _test_cuda_functionality() -> bool:
    """
    Tests if CUDA is actually functional, not just available.

    Returns:
        bool: True if CUDA works, False otherwise.
    """
    if not torch.cuda.is_available():
        return False

    try:
        test_tensor = torch.tensor([1.0])
        test_tensor = test_tensor.cuda()
        test_tensor = test_tensor.cpu()
        return True
    except Exception as e:
        logger.warning(f"CUDA functionality test failed: {e}")
        return False


def _test_mps_functionality() -> bool:
    """
    Tests if MPS is actually functional, not just available.

    Returns:
        bool: True if MPS works, False otherwise.
    """
    if not torch.backends.mps.is_available():
        return False

    try:
        test_tensor = torch.tensor([1.0])
        test_tensor = test_tensor.to("mps")
        test_tensor = test_tensor.cpu()
        return True
    except Exception as e:
        logger.warning(f"MPS functionality test failed: {e}")
        return False


def load_model() -> bool:
    """
    Loads the multilingual TTS model by default.
    """
    global chatterbox_model, MODEL_LOADED, model_device, multilingual_model, MULTILINGUAL_MODEL_LOADED
    global vc_model, VC_MODEL_LOADED  # â† Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐž: Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð»Ñ VC

    if MODEL_LOADED:
        logger.info("TTS model is already loaded.")
        return True

    try:
        # â†“â†“â†“ ÐŸÐžÐ›Ð£Ð§ÐÐ•Ðœ ÐŸÐ£Ð¢Ð¬ Ðš ÐšÐ­Ð¨Ð£ ÐÐÐŸÐ Ð¯ÐœÐ£Ð® Ð˜Ð— CONFIG_MANAGER â†“â†“â†“
        model_cache_path = config_manager.get_path("paths.model_cache", "./model_cache", ensure_absolute=True)
        logger.info(f"ðŸ“ ÐŸÑƒÑ‚ÑŒ Ðº ÐºÑÑˆÑƒ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: {model_cache_path}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ñ€Ð°Ð·Ð¼ÐµÑ€ ÐºÑÑˆÐ°
        from pathlib import Path
        cache_path = Path(model_cache_path)
        if cache_path.exists():
            # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° ÐºÑÑˆÐ°
            total_size = 0
            file_count = 0
            for file_path in cache_path.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
                    file_count += 1
            
            size_gb = total_size / (1024**3)
            size_mb = total_size / (1024**2)
            
            logger.info(f"ðŸ“¦ Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÑÑˆÐ°: {size_gb:.2f} GB ({size_mb:.0f} MB)")
            logger.info(f"ðŸ“„ Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð² ÐºÑÑˆÐµ: {file_count}")
            
            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² ÐºÑÑˆÐµ
            model_dirs = [d for d in cache_path.iterdir() if d.is_dir() and d.name.startswith("models--")]
            logger.info(f"ðŸ“š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² ÐºÑÑˆÐµ: {len(model_dirs)}")
            
            for model_dir in model_dirs:
                model_name = model_dir.name.replace("models--", "").replace("--", "/")
                
                # Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
                model_size = sum(f.stat().st_size for f in model_dir.rglob("*") if f.is_file())
                model_size_mb = model_size / (1024**2)
                
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ snapshots
                snapshots_dir = model_dir / "snapshots"
                if snapshots_dir.exists():
                    snapshots = list(snapshots_dir.iterdir())
                    if snapshots:
                        snapshot = snapshots[0]  # Ð±ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ snapshot
                        snapshot_files = list(snapshot.rglob("*.*"))
                        logger.info(f"  â””â”€ {model_name}: {model_size_mb:.1f} MB, {len(snapshot_files)} Ñ„Ð°Ð¹Ð»Ð¾Ð²")
                        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
                        for file_path in snapshot_files[:3]:  # Ð¿ÐµÑ€Ð²Ñ‹Ðµ 3 Ñ„Ð°Ð¹Ð»Ð°
                            if file_path.is_file():
                                file_mb = file_path.stat().st_size / (1024**2)
                                logger.info(f"     â€¢ {file_path.name}: {file_mb:.1f} MB")
                else:
                    logger.info(f"  â””â”€ {model_name}: {model_size_mb:.1f} MB (ÑÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÑ‚ÑÑ...)")
        else:
            logger.info("ðŸ“­ ÐšÑÑˆ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚, Ð±ÑƒÐ´ÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½ Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹")
        # â†‘â†‘â†‘ ÐŸÐžÐ›Ð£Ð§ÐÐ•Ðœ ÐŸÐ£Ð¢Ð¬ Ðš ÐšÐ­Ð¨Ð£ ÐÐÐŸÐ Ð¯ÐœÐ£Ð® Ð˜Ð— CONFIG_MANAGER â†‘â†‘â†‘

        # Determine the device
        device_setting = config_manager.get_string("tts_engine.device", "auto")
        if device_setting == "auto":
            if _test_cuda_functionality():
                resolved_device_str = "cuda"
                logger.info("CUDA functionality test passed. Using CUDA.")
            elif _test_mps_functionality():
                resolved_device_str = "mps"
                logger.info("MPS functionality test passed. Using MPS.")
            else:
                resolved_device_str = "cpu"
                logger.info("CUDA and MPS not functional or not available. Using CPU.")

        elif device_setting == "cuda":
            if _test_cuda_functionality():
                resolved_device_str = "cuda"
                logger.info("CUDA requested and functional. Using CUDA.")
            else:
                resolved_device_str = "cpu"
                logger.warning(
                    "CUDA was requested in config but functionality test failed. "
                    "PyTorch may not be compiled with CUDA support. "
                    "Automatically falling back to CPU."
                )

        elif device_setting == "mps":
            if _test_mps_functionality():
                resolved_device_str = "mps"
                logger.info("MPS requested and functional. Using MPS.")
            else:
                resolved_device_str = "cpu"
                logger.warning(
                    "MPS was requested in config but functionality test failed. "
                    "PyTorch may not be compiled with MPS support. "
                    "Automatically falling back to CPU."
                )

        elif device_setting == "cpu":
            resolved_device_str = "cpu"
            logger.info("CPU device explicitly requested in config. Using CPU.")

        else:
            logger.warning(
                f"Invalid device setting '{device_setting}' in config. "
                f"Defaulting to auto-detection."
            )
            if _test_cuda_functionality():
                resolved_device_str = "cuda"
            elif _test_mps_functionality():
                resolved_device_str = "mps"
            else:
                resolved_device_str = "cpu"
            logger.info(f"Auto-detection resolved to: {resolved_device_str}")
        model_device = resolved_device_str
        logger.info(f"ðŸŽ¯ Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: {model_device}")

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ TTS Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        logger.info("â¬‡ï¸  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° TTS Ð¼Ð¾Ð´ÐµÐ»Ð¸...")
        multilingual_model = PatchedChatterboxTTS.from_pretrained(device=model_device)
        chatterbox_model = multilingual_model
        MULTILINGUAL_MODEL_LOADED = True
        MODEL_LOADED = True

        logger.info(f"âœ… TTS Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {model_device}")
        logger.info("ðŸŒ ÐœÑƒÐ»ÑŒÑ‚Ð¸ÑÐ·Ñ‹Ñ‡Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð’Ð¡Ð•Ð¥ ÑÐ·Ñ‹ÐºÐ¾Ð².")
        
        # â†“â†“â†“ Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐž: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Voice Conversion â†“â†“â†“
        try:
            # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ VC Ð¼Ð¾Ð´ÐµÐ»ÑŒ
            logger.info("â¬‡ï¸  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Voice Conversion Ð¼Ð¾Ð´ÐµÐ»Ð¸...")
            vc_model = ChatterboxVC.from_pretrained(device=model_device)
            VC_MODEL_LOADED = True
            logger.info(f"âœ… Voice Conversion Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {model_device}")
            
            # â†“â†“â†“ Ð’Ð«Ð’ÐžÐ” Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð˜ Ðž Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐÐ«Ð¥ ÐœÐžÐ”Ð•Ð›Ð¯Ð¥ â†“â†“â†“
            logger.info("=" * 60)
            logger.info("ðŸ“Š Ð¡Ð¢ÐÐ¢Ð£Ð¡ Ð—ÐÐ“Ð Ð£Ð—ÐšÐ˜ ÐœÐžÐ”Ð•Ð›Ð•Ð™:")
            logger.info(f"  â€¢ TTS Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {'âœ… Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ' if MODEL_LOADED else 'âŒ ÐÐ• Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ'}")
            logger.info(f"  â€¢ VC Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {'âœ… Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ' if VC_MODEL_LOADED else 'âŒ ÐÐ• Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ'}")
            logger.info(f"  â€¢ Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {model_device}")
            logger.info(f"  â€¢ ÐŸÑƒÑ‚ÑŒ ÐºÑÑˆÐ°: {model_cache_path}")
            
            # ÐŸÐ°Ð¼ÑÑ‚ÑŒ GPU
            if model_device == "cuda" and torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0)
                total_memory = gpu_memory.total_memory / (1024**3)
                allocated_memory = torch.cuda.memory_allocated() / (1024**3)
                reserved_memory = torch.cuda.memory_reserved() / (1024**3)
                
                logger.info(f"  â€¢ GPU Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {allocated_memory:.1f}/{total_memory:.1f} GB Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¾")
                logger.info(f"  â€¢ GPU Ð·Ð°Ñ€ÐµÐ·ÐµÑ€Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾: {reserved_memory:.1f} GB")
            
            logger.info("=" * 60)
            # â†‘â†‘â†‘ Ð’Ð«Ð’ÐžÐ” Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð˜ Ðž Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐÐ«Ð¥ ÐœÐžÐ”Ð•Ð›Ð¯Ð¥ â†‘â†‘â†‘
            
        except Exception as vc_e:
            logger.warning(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Voice Conversion Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {vc_e}")
            logger.warning("Ð’ÐºÐ»Ð°Ð´ÐºÐ° Voice Conversion Ð±ÑƒÐ´ÐµÑ‚ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°.")
            vc_model = None
            VC_MODEL_LOADED = False
            
            # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ VC Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ð»Ð°ÑÑŒ
            logger.info("=" * 60)
            logger.info("ðŸ“Š Ð¡Ð¢ÐÐ¢Ð£Ð¡ Ð—ÐÐ“Ð Ð£Ð—ÐšÐ˜ ÐœÐžÐ”Ð•Ð›Ð•Ð™:")
            logger.info(f"  â€¢ TTS Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {'âœ… Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ' if MODEL_LOADED else 'âŒ ÐÐ• Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ'}")
            logger.info(f"  â€¢ VC Ð¼Ð¾Ð´ÐµÐ»ÑŒ: âŒ ÐÐ• Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐ (Ð¾ÑˆÐ¸Ð±ÐºÐ°: {str(vc_e)[:100]}...)")
            logger.info(f"  â€¢ Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {model_device}")
            logger.info(f"  â€¢ ÐŸÑƒÑ‚ÑŒ ÐºÑÑˆÐ°: {model_cache_path}")
            logger.info("=" * 60)
        # â†‘â†‘â†‘ Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐž: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Voice Conversion â†‘â†‘â†‘

        return True

    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸ÑÐ·Ñ‹Ñ‡Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {e}", exc_info=True)
        multilingual_model = None
        chatterbox_model = None
        MULTILINGUAL_MODEL_LOADED = False
        MODEL_LOADED = False
        return False


def get_model_info() -> dict:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ….
    ÐœÐ¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð²Ñ‹Ð·Ð²Ð°Ð½Ð° Ð¸Ð· server_gradio.py Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐµ.
    """
    try:
        model_cache_path = config_manager.get_path("paths.model_cache", "./model_cache", ensure_absolute=True)
        cache_path = Path(model_cache_path)
        
        info = {
            "status": {
                "tts_loaded": MODEL_LOADED,
                "vc_loaded": VC_MODEL_LOADED if 'VC_MODEL_LOADED' in globals() else False,
                "device": model_device,
                "cache_path": str(model_cache_path)
            },
            "cache_info": {
                "exists": cache_path.exists(),
                "total_size_mb": 0,
                "model_count": 0,
                "models": []
            }
        }
        
        if cache_path.exists():
            # Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÑÑˆÐ°
            total_size = sum(f.stat().st_size for f in cache_path.rglob("*") if f.is_file())
            info["cache_info"]["total_size_mb"] = total_size / (1024**2)
            
            # ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð² ÐºÑÑˆÐµ
            model_dirs = [d for d in cache_path.iterdir() if d.is_dir() and d.name.startswith("models--")]
            info["cache_info"]["model_count"] = len(model_dirs)
            
            for model_dir in model_dirs:
                model_name = model_dir.name.replace("models--", "").replace("--", "/")
                model_size = sum(f.stat().st_size for f in model_dir.rglob("*") if f.is_file())
                
                model_info = {
                    "name": model_name,
                    "size_mb": model_size / (1024**2),
                    "path": str(model_dir)
                }
                
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ snapshots
                snapshots_dir = model_dir / "snapshots"
                if snapshots_dir.exists():
                    snapshots = list(snapshots_dir.iterdir())
                    if snapshots:
                        snapshot = snapshots[0]
                        model_info["snapshot"] = str(snapshot)
                        model_info["files"] = [f.name for f in snapshot.iterdir() if f.is_file()][:5]
                
                info["cache_info"]["models"].append(model_info)
        
        return info
        
    except Exception as e:
        return {"error": str(e)}


def load_multilingual_model() -> bool:
    """
    Loads the multilingual TTS model, unloads the standard model,
    and sets the multilingual model as the default for all languages.
    """
    global multilingual_model, MULTILINGUAL_MODEL_LOADED, model_device
    global chatterbox_model, MODEL_LOADED

    if MULTILINGUAL_MODEL_LOADED:
        logger.info("Multilingual TTS model is already loaded and set as default.")
        return True

    if model_device is None:
        logger.error("Main model device not determined. Load main model first.")
        return False

    if chatterbox_model is not None:
        logger.info("Unloading the standard ChatterboxTTS model to free up memory...")
        chatterbox_model = None
        MODEL_LOADED = False
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        logger.info("Standard model unloaded and memory cleared.")

    try:
        logger.info(f"Loading multilingual model (PatchedChatterboxTTS) on {model_device}...")

        multilingual_model = PatchedChatterboxTTS.from_pretrained(device=model_device)

        chatterbox_model = multilingual_model

        MULTILINGUAL_MODEL_LOADED = True
        MODEL_LOADED = True

        logger.info(f"PatchedChatterboxTTS model loaded successfully on {model_device}.")
        logger.info("This model will now be used for ALL languages, including English.")
        return True

    except Exception as e:
        logger.error(f"Error loading multilingual model: {e}", exc_info=True)
        multilingual_model = None
        chatterbox_model = None
        MULTILINGUAL_MODEL_LOADED = False
        MODEL_LOADED = False
        return False
    

def load_vc_model() -> bool:
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Voice Conversion.
    """
    global vc_model, VC_MODEL_LOADED, model_device
    
    if VC_MODEL_LOADED:
        logger.info("Voice Conversion model is already loaded.")
        return True
    
    if model_device is None:
        logger.error("Main model device not determined. Load main model first.")
        return False
    
    try:
        logger.info(f"Loading Voice Conversion model on {model_device}...")
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ VC
        vc_model = ChatterboxVC.from_pretrained(device=model_device)
        VC_MODEL_LOADED = True
        
        logger.info(f"Voice Conversion model loaded successfully on {model_device}.")
        return True
        
    except Exception as e:
        logger.error(f"Error loading Voice Conversion model: {e}", exc_info=True)
        vc_model = None
        VC_MODEL_LOADED = False
        return False


def get_or_load_vc_model() -> Optional[ChatterboxVC]:
    """
    ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð¸Ð»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Voice Conversion.
    """
    global vc_model, VC_MODEL_LOADED
    
    if not VC_MODEL_LOADED:
        if not load_vc_model():
            return None
    
    return vc_model



    
def synthesize(
    text: str,
    audio_prompt_path: Optional[str] = None,
    temperature: float = 0.8,
    exaggeration: float = 0.5,
    cfg_weight: float = 0.5,
    seed: int = 0,
    language: str = "en",
) -> Tuple[Optional[torch.Tensor], Optional[int]]:
    """
    Synthesizes audio from text using the currently loaded TTS model.
    If the multilingual model is loaded, it handles all languages.
    """
    global chatterbox_model, multilingual_model

    if not MODEL_LOADED or chatterbox_model is None:
        logger.error("TTS model is not loaded. Cannot synthesize audio.")
        return None, None
    
    active_model = chatterbox_model

    try:
        if seed != 0:
            logger.info(f"Applying user-provided seed for generation: {seed}")
            set_seed(seed)
        else:
            logger.info("Using default (potentially random) generation behavior as seed is 0.")

        logger.debug(
            f"Synthesizing with params: audio_prompt='{audio_prompt_path}', temp={temperature}, "
            f"exag={exaggeration}, cfg_weight={cfg_weight}, seed_applied_globally_if_nonzero={seed}, "
            f"language={language}"
        )

        is_multilingual = isinstance(active_model, ChatterboxMultilingualTTS)
        
        if is_multilingual:
            logger.info(f"Synthesizing with multilingual model for language: {language}")
            wav_tensor = active_model.generate(
                text=text,
                audio_prompt_path=audio_prompt_path,
                temperature=temperature,
                exaggeration=exaggeration,
                cfg_weight=cfg_weight,
                language_id=language,
            )
        else:
            logger.info("Synthesizing with standard English model.")
            wav_tensor = active_model.generate(
                text=text,
                audio_prompt_path=audio_prompt_path,
                temperature=temperature,
                exaggeration=exaggeration,
                cfg_weight=cfg_weight,
            )

        return wav_tensor, active_model.sr

    except Exception as e:
        logger.error(f"Error during TTS synthesis: {e}", exc_info=True)
        return None, None


def get_supported_languages() -> list:
    """
    Returns a list of all supported languages â€‹â€‹for the UI.
    """
    langs = SUPPORTED_LANGUAGES
    if isinstance(langs, dict):
        return list(langs.keys())
    elif isinstance(langs, (list, tuple, set)):
        return list(langs)
    else:
        try:
            return list(getattr(langs, "keys")())
        except Exception:
            return ["en"]
